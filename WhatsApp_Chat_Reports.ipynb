{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BakuDoriya-KatsZuku/Whatsapp_Chat_Reports/blob/main/WhatsApp_Chat_Reports.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "import datetime\n",
        "from textblob import TextBlob\n",
        "import plotly.graph_objs as go\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "with open(\"/content/chat_filename\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "dates, times, names, messages = [], [], [], []\n",
        "\n",
        "for line in lines:\n",
        "    line.strip(\"\\n\")\n",
        "    if \" - \" in line:\n",
        "        parts = line.split(\" - \")\n",
        "        if len(parts) == 2:\n",
        "            date_time = parts[0].strip().split(', ')\n",
        "            if len(date_time) == 2:\n",
        "                date, time = date_time\n",
        "                x = parts[1].split(\":\")\n",
        "                name = ''\n",
        "                if len(x) == 2:\n",
        "                  name, message = x\n",
        "                  dates.append(date)\n",
        "                  times.append(time)\n",
        "                  names.append(name)\n",
        "                  messages.append(message)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"Date\": dates,\n",
        "    \"Time\": times,\n",
        "    \"Name\": names,\n",
        "    \"Message\": messages\n",
        "})\n",
        "\n",
        "df[\"Message\"] = df[\"Message\"].str.strip(\"\\n\")"
      ],
      "metadata": {
        "id": "NcvYWx7iLfq6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = dict(df[\"Name\"].value_counts())\n",
        "x = pd.DataFrame(x.items(), columns = [\"Name\", \"Count\"])\n",
        "fig = px.bar(x, x = \"Name\", y = \"Count\", title = \"Number of Messages\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "FikRkQthVZbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make DF words and count (Keeping this so that we can also use for plotting)\n",
        "x = {}\n",
        "for i in range(df.shape[0]):\n",
        "  words = df[\"Message\"].iloc[i].split()\n",
        "  for word in words:\n",
        "    if word not in x:\n",
        "      x[word] = 1\n",
        "    else:\n",
        "      x[word] += 1\n",
        "x = pd.DataFrame(x.items(), columns = [\"Word\", \"Count\"])\n",
        "\n",
        "#Removing Media\n",
        "z = x[(x[\"Word\"] == \"<Media\") | (x[\"Word\"] == \"omitted>\")].index\n",
        "x.drop(z, inplace = True)\n",
        "x = x.sort_values(by = 'Count', ascending = False)\n",
        "x = x.loc[x[\"Count\"] >= 10]\n",
        "print(\"Most Messaged Words\")\n",
        "print(x.to_string(index = False))"
      ],
      "metadata": {
        "id": "6_JbwoORd15Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making Dictionary for words with lowercase (IDK why groupby does not give result. It just prints unkown characters)\n",
        "x = {}\n",
        "for i in range(df.shape[0]):\n",
        "  words = df[\"Message\"].iloc[i].split()\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    if word not in x:\n",
        "      x[word] = 1\n",
        "    else:\n",
        "      x[word] += 1\n",
        "x = pd.DataFrame(x.items(), columns = [\"Word\", \"Count\"])\n",
        "x = x.sort_values(by = 'Count', ascending = False)\n",
        "\n",
        "print(x.loc[x[\"Word\"].isin(['bsdk','fuck', 'mc', 'bc', \"f\"])])"
      ],
      "metadata": {
        "id": "6zPP2AfasPIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = df[df.Message != \"<Media omitted>\"] #Removing Medai ommitted so that they do not hamper word counts\n",
        "\n",
        "z = df[\"Name\"].unique() #List of unique names\n",
        "\n",
        "#Dictionary to count the number of words in each name\n",
        "x = {}\n",
        "for name in z:\n",
        "  x[name] = 0\n",
        "for i in range(new_df.shape[0]):\n",
        "  x[df[\"Name\"].iloc[i]] += len(new_df[\"Message\"].iloc[i].split())\n",
        "\n",
        "#Sortig the Dictionary in reverse order\n",
        "keys = list(x.keys())\n",
        "values = list(x.values())\n",
        "sorted_value_index = np.argsort(values)\n",
        "x = {keys[i]: values[i] for i in sorted_value_index}\n",
        "x = dict(reversed(list(x.items())))\n",
        "\n",
        "#Plot\n",
        "x = pd.DataFrame(x.items(), columns = [\"Name\", \"Count\"])\n",
        "fig = px.bar(x, x = \"Name\", y = \"Count\", title = \"Number of Words\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "G-w0hPTGbzBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Date'] = pd.to_datetime(df['Date'], dayfirst=True) #Forcing all the values in the column to become in that format else NAN\n",
        "if df['Date'].isnull().sum() > 0:\n",
        "    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%d-%m', errors='coerce')\n",
        "df[\"Date\"] = df[\"Date\"].dropna() #Drop all useless values\n",
        "df['Month_Year'] = df['Date'].dt.strftime('%B %Y') #Converting to Month year for better looking on the graph\n",
        "grouped = df.groupby(['Name', 'Month_Year', 'Date']).count() #Grouping these values ie making them the primary key\n",
        "\n",
        "# Performing Operations for each and every person\n",
        "for name in grouped.index.levels[0]:\n",
        "    sub_df = grouped.loc[name].reset_index()\n",
        "    sub_df = sub_df.sort_values(by='Date')\n",
        "    #Replacing the missing date values ie where the person did not message and putting the message count to 0 over there\n",
        "    sub_df = sub_df.set_index(\"Date\")\n",
        "    sub_df = sub_df.resample(\"D\").asfreq() #Replace missing date values (Can change only if index is the column whose dates you want to insert ie all the values there have to be unique)\n",
        "    sub_df[\"Message\"].fillna(0, inplace=True)\n",
        "    sub_df = sub_df.reset_index() #Change index again\n",
        "    #Plot\n",
        "    fig = px.line(sub_df, x = 'Date', y = 'Message', title = name)\n",
        "    fig.show()\n",
        "\n",
        "#Same but not bringing back in the dates (Can't do because they repeat because of repeating name. One way is the create different dataframes and then concat them and then plot with color = \"name\" but takes too much time so did not do it)\n",
        "grouped = grouped.reset_index()\n",
        "grouped = grouped.sort_values(by='Date')\n",
        "fig = px.line(grouped, x = 'Date', y = 'Message', color = 'Name', title = 'Everyone')\n",
        "fig.show()\n",
        "\n",
        "#Same thing but for the whole group so ignoring the names\n",
        "grouped = df.groupby(['Date']).count()\n",
        "grouped = grouped.reset_index()\n",
        "grouped = grouped.sort_values(by = 'Date')\n",
        "grouped = grouped.set_index(\"Date\")\n",
        "grouped = grouped.resample(\"D\").asfreq()\n",
        "grouped[\"Message\"].fillna(0, inplace=True)\n",
        "grouped = grouped.reset_index()\n",
        "fig = px.line(grouped, x = 'Date', y = 'Message', title = 'Overall messages of the group')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "DSdH4mPMh1x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using textblob for sentimental analysis (IDK how to use it so chatGPT(Orz))\n",
        "df['Sentiment'] = df['Message'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "grouped = df.groupby(['Name'])\n",
        "sentiment_scores = grouped['Sentiment'].mean() #Finding mean because just 1 number of each person look good\n",
        "\n",
        "#Plot\n",
        "fig = px.bar(sentiment_scores, x=sentiment_scores.index, y=sentiment_scores, title='Sentiment Scores by Person')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "rCxmXMDk2AOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instead of messages did the same process for number of words\n",
        "df['Word Count'] = df['Message'].apply(lambda x: len(x.split()))\n",
        "grouped = df.groupby(['Date', 'Name'])\n",
        "word_count_per_day = grouped['Word Count'].sum().reset_index()\n",
        "\n",
        "for name, sub_df in word_count_per_day.groupby('Name'):\n",
        "    sub_df = sub_df.sort_values(by='Date')\n",
        "    sub_df = sub_df.set_index(\"Date\")\n",
        "    sub_df = sub_df.resample(\"D\").asfreq()\n",
        "    sub_df[\"Word Count\"].fillna(0, inplace=True)\n",
        "    sub_df = sub_df.reset_index()\n",
        "    fig = px.line(sub_df, x='Date', y='Word Count', title=name)\n",
        "    fig.show()\n",
        "\n",
        "overall_word_count = word_count_per_day.groupby('Date').sum().reset_index()\n",
        "overall_word_count = overall_word_count.sort_values(by='Date')\n",
        "overall_word_count = overall_word_count.set_index(\"Date\")\n",
        "overall_word_count = overall_word_count.resample(\"D\").asfreq()\n",
        "overall_word_count[\"Word Count\"].fillna(0, inplace=True)\n",
        "overall_word_count = overall_word_count.reset_index()\n",
        "\n",
        "fig = px.line(overall_word_count, x='Date', y='Word Count', title='Overall Word Count of the group')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "5BeJqhMO455R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instead of Date using hours as a metric to see which time of the day is most active\n",
        "def convert(string):\n",
        "      if string[-2:] == \"am\" and string[:2] == \"12\":\n",
        "         return \"00\" + string[2:5]\n",
        "      elif string[-2:] == \"am\":\n",
        "         return string[:2]\n",
        "      elif string[-2:] == \"pm\" and string[:2] == \"12\":\n",
        "         return string[:5]\n",
        "      elif string[1] == ':':\n",
        "          return str(int(string[:1]) + 12) + string[1:5]\n",
        "      else:\n",
        "          return str(int(string[:2]) + 12) + string[2:5]\n",
        "\n",
        "def extract_hour(time_str):\n",
        "    if time_str[-2:] >= \"30\":\n",
        "      if time_str[:2] == \"19\":\n",
        "        return 20\n",
        "      elif time_str[0] == '9':\n",
        "        return 10\n",
        "      elif time_str[:2] == '23':\n",
        "        return 0\n",
        "      elif time_str[1] == ':':\n",
        "        return (int(time_str[0]) + 1)\n",
        "      else:\n",
        "        return (int(time_str[:2]) + 1)\n",
        "    else:\n",
        "      if  time_str[1] == \":\":\n",
        "        return int(time_str[0])\n",
        "      else:\n",
        "        return int(time_str[:2])\n",
        "\n",
        "df[\"Hour\"] = df[\"Time\"].apply(convert)\n",
        "df['Hour'] = df['Hour'].apply(extract_hour) #Making new column of hours\n",
        "grouped = df.groupby([\"Name\", \"Hour\"]).count()\n",
        "\n",
        "#Individual\n",
        "all_hours = range(24)\n",
        "for name in grouped.index.levels[0]:\n",
        "  sub_df = grouped.loc[name].reset_index()\n",
        "  sub_df = sub_df.sort_values(by = \"Hour\")\n",
        "  missing_hours = pd.Series(all_hours).isin(sub_df['Hour']).apply(lambda x: not x)\n",
        "  sub_df = sub_df.append(pd.DataFrame({'Hour': missing_hours[missing_hours].index}))\n",
        "  sub_df = sub_df.sort_values('Hour').reset_index(drop=True)\n",
        "  sub_df[\"Message\"] = sub_df[\"Message\"].fillna(0)\n",
        "  fig = px.line(sub_df, x = 'Hour', y = 'Message', title = name)\n",
        "  fig.update_layout(xaxis=dict(dtick=1))\n",
        "  fig.show()\n",
        "\n",
        "#Whole Group\n",
        "grouped = df.groupby(\"Hour\")[\"Message\"].size().reset_index()\n",
        "all_hours = pd.DataFrame({'Hour': range(24)})\n",
        "grouped = all_hours.merge(grouped, on='Hour', how='left').fillna(0)\n",
        "fig = px.line(grouped, x = \"Hour\", y = \"Message\", title = \"All Messages of the group\")\n",
        "fig.update_layout(xaxis=dict(dtick=1))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "P2JQq_eUZGe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def awc(x, y):\n",
        "  for name in list(x.keys()):\n",
        "    x[name] = (x[name] / y[name])\n",
        "  return x\n",
        "\n",
        "y = dict(df[\"Name\"].value_counts())\n",
        "\n",
        "new_df = df[df.Message != \"<Media omitted>\"] #Removing Medai ommitted so that they do not hamper word counts\n",
        "\n",
        "z = df[\"Name\"].unique() #List of unique names\n",
        "\n",
        "#Dictionary to count the number of words in each name\n",
        "x = {}\n",
        "for name in z:\n",
        "  x[name] = 0\n",
        "for i in range(new_df.shape[0]):\n",
        "  x[df[\"Name\"].iloc[i]] += len(new_df[\"Message\"].iloc[i].split())\n",
        "\n",
        "out = awc(x, y)\n",
        "\n",
        "keys = list(x.keys())\n",
        "values = list(x.values())\n",
        "sorted_value_index = np.argsort(values)\n",
        "x = {keys[i]: values[i] for i in sorted_value_index}\n",
        "x = dict(reversed(list(x.items())))\n",
        "\n",
        "print(\"Average Words per Message\\n\")\n",
        "keys = list(x.keys())\n",
        "for name in keys:\n",
        "  print(name + \": \" + str(x[name]) + \"\\n\")"
      ],
      "metadata": {
        "id": "mO1L2sV8pxCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Day\"] = df[\"Date\"].dt.day_name()\n",
        "\n",
        "grouped = df.groupby([\"Day\", \"Hour\"]).count()\n",
        "cat_dtype = pd.CategoricalDtype(categories=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], ordered=True)\n",
        "grouped.index.set_levels(grouped.index.levels[0].astype(cat_dtype), level=0, inplace=True)\n",
        "grouped = grouped.sort_values(\"Day\")\n",
        "grouped = grouped.reset_index()\n",
        "\n",
        "fig = px.bar(grouped, x=\"Hour\", y=\"Message\", color=\"Day\", title=\"Daily Number of message by Hour\", color_discrete_sequence=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2'])\n",
        "fig.show()\n",
        "\n",
        "fig = px.histogram(grouped, x=\"Hour\", y=\"Message\", color='Day', barmode='group', title = \"Better Plot\", color_discrete_sequence=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2'])\n",
        "fig.show()\n",
        "\n",
        "#Different color options\n",
        "# fig = px.histogram(grouped, x=\"Hour\", y=\"Message\", color='Day', barmode='group', color_discrete_sequence=['#FF6361', '#58508d', '#bc5090', '#FFA600', '#003f5c', '#7f33a6', '#FFD700'])\n",
        "# fig = px.histogram(grouped, x=\"Hour\", y=\"Message\", color='Day', barmode='group', color_discrete_sequence=['#f2a154', '#e74c3c', '#c24e3c', '#f7dc6f', '#d1ccc0', '#e67e22', '#f1c40f'])\n",
        "# fig = px.histogram(grouped, x=\"Hour\", y=\"Message\", color='Day', barmode='group', color_discrete_sequence=['#4B778D', '#4FB99F', '#F9CDAD', '#C6D8D3', '#F6BD60', '#FFE66D', '#F16B6F'])\n",
        "# fig = px.histogram(grouped, x=\"Hour\", y=\"Message\", color='Day', barmode='group', color_discrete_sequence=['#F7A278', '#F4B4B4', '#A2B8C4', '#D9CAB3', '#F9DAD0', '#A4D4AE', '#D4A4A4'])\n",
        "# fig = px.histogram(grouped, x=\"Hour\", y=\"Message\", color='Day', barmode='group', color_discrete_sequence=['#9e7a4c', '#6b4e31', '#5f5b3f', '#85815e', '#bd9e39', '#d2b48c', '#cd853f'])\n",
        "# fig = px.histogram(grouped, x=\"Hour\", y=\"Message\", color='Day', barmode='group', color_discrete_sequence=['#e6194B', '#3cb44b', '#f58231', '#911eb4', '#ffe119', '#42d4f4', '#f032e6'])"
      ],
      "metadata": {
        "id": "PQ43vzqR6e90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy\n",
        "\n",
        "newdf = df[df.Message != \"<Media omitted>\"]\n",
        "\n",
        "X = newdf['Message']\n",
        "y = newdf['Name']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "\n",
        "clf1 = MultinomialNB()\n",
        "clf1.fit(X_train_vec, y_train)\n",
        "\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "y_pred = clf1.predict(X_test_vec)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: {}\".format(accuracy))"
      ],
      "metadata": {
        "id": "LGwXJd3hmfQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Only based on Message data\n",
        "\n",
        "# newdf = df[df.Message != \"<Media omitted>\"]\n",
        "\n",
        "# X = newdf['Message']\n",
        "# y = newdf['Name']\n",
        "\n",
        "# #Vectorize the text data\n",
        "# vectorizer = CountVectorizer()\n",
        "# X_vec = vectorizer.fit_transform(X)\n",
        "\n",
        "# clf1 = MultinomialNB()\n",
        "# clf1.fit(X_vec, y)"
      ],
      "metadata": {
        "id": "eyHZ03v47mSa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #All, Message, hour and day\n",
        "\n",
        "# newdf = df[df.Message != \"<Media omitted>\"]\n",
        "\n",
        "# X = newdf[['Message', 'Day', 'Hour']]\n",
        "# y = newdf['Name']\n",
        "\n",
        "# X = pd.get_dummies(X, columns=['Day']) #Convert categorical data to numerical data\n",
        "\n",
        "# #Vectorize the text data\n",
        "# vectorizer = CountVectorizer()\n",
        "# X_vec = vectorizer.fit_transform(X['Message'])\n",
        "\n",
        "# X_vec_df = pd.DataFrame(X_vec.toarray()) #Create a DataFrame of the vectorized messages\n",
        "\n",
        "# #Add column names to the feature matrix\n",
        "# X_new = pd.concat([X_vec_df, X[['Hour', 'Day_Friday', 'Day_Monday', 'Day_Saturday', 'Day_Sunday', 'Day_Thursday', 'Day_Tuesday', 'Day_Wednesday']].reset_index(drop=True)], axis=1)\n",
        "# X_new.columns = X_new.columns.astype(str)\n",
        "\n",
        "# #Train the model\n",
        "# clf3 = MultinomialNB()\n",
        "# clf3.fit(X_new, y)"
      ],
      "metadata": {
        "id": "oe4f7-fZoACt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def predict_sender3(msg, hour, day):\n",
        "#     msg_vec = vectorizer.transform([msg])\n",
        "\n",
        "#     hour_day = pd.get_dummies(pd.DataFrame({'Hour': [hour], 'Day': [day]}), columns=['Hour', 'Day'])\n",
        "\n",
        "#     #Add the Hour and Day columns in the feature matrix and reorder the columns to match the order in X_new\n",
        "#     msg_vec = pd.concat([pd.DataFrame(msg_vec.toarray()), hour_day], axis=1)\n",
        "#     msg_vec = msg_vec.reindex(columns=X_new.columns, fill_value=0)\n",
        "#     msg_vec.columns = msg_vec.columns.astype(str)\n",
        "\n",
        "#     sender = clf3.predict(msg_vec)[0]\n",
        "\n",
        "#     #Predict the probabilities of all unique names\n",
        "#     probas = clf3.predict_proba(msg_vec)[0]\n",
        "#     prob_dict = dict(zip(clf3.classes_, probas))\n",
        "\n",
        "#     sorted_probas = sorted(prob_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "#     print(\"Predicted Sender: {}\\n\".format(sender))\n",
        "#     for name, proba in sorted_probas:\n",
        "#         print(\"- {}: {:.2f}%\".format(name, proba*100))\n",
        "\n",
        "def predict_sender1(msg):\n",
        "    msg_vec = vectorizer.transform([msg])\n",
        "\n",
        "    sender = clf1.predict(msg_vec)[0]\n",
        "\n",
        "    # Predict the probabilities of all unique names\n",
        "    probas = clf1.predict_proba(msg_vec)[0]\n",
        "    prob_dict = dict(zip(clf1.classes_, probas))\n",
        "\n",
        "    sorted_probas = sorted(prob_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "    print(\"Text: {}\\n\".format(msg))\n",
        "    print(\"Predicted Sender: {}\\n\".format(sender))\n",
        "    for name, proba in sorted_probas:\n",
        "        print(\"- {}: {:.2f}%\".format(name, proba*100))\n",
        "\n",
        "\n",
        "msg = \"For survival hanging out together is necessary innit.\"\n",
        "# hour = 18\n",
        "# day = \"Thursday\"\n",
        "# print(\"Message: {}\\nHour: {}\\nDay: {}\\n\\n\".format(msg, hour, day))\n",
        "# print(\"Taking into consideration hour, day and message\")\n",
        "# predict_sender3(msg, hour, day)\n",
        "#print(\"\\n\\nTaking into consideration only message\")\n",
        "predict_sender1(msg)"
      ],
      "metadata": {
        "id": "lwSd0DbMoo44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy\n",
        "\n",
        "newdf = df[df.Message != \"<Media omitted>\"]\n",
        "newdf = newdf[newdf.Message != \" <Media omitted>\"]\n",
        "\n",
        "messages = newdf['Message']\n",
        "senders = newdf['Name']\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(messages)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, senders, test_size=0.2, random_state=42)\n",
        "\n",
        "classifier = RandomForestClassifier()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "nQb4WHPFED9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sender_probability(text):\n",
        "    text_vectorized = vectorizer.transform([text])\n",
        "    probabilities = classifier.predict_proba(text_vectorized)[0]\n",
        "    sender_probabilities = dict(zip(classifier.classes_, probabilities))\n",
        "    sender_probabilities = sorted(sender_probabilities.items(), key=lambda x: x[1], reverse = True)\n",
        "\n",
        "    print(\"Text: {}\\n\".format(text))\n",
        "    print(\"Predicted Sender: {}\\n\".format(sender_probabilities[0][0]))\n",
        "    for key, value in sender_probabilities:\n",
        "      print(\"- {}: {:.2f}%\".format(key, value*100))\n",
        "\n",
        "input_text = \"For survival hanging out together is necessary innit.\"\n",
        "predict_sender_probability(input_text)"
      ],
      "metadata": {
        "id": "QDH-CHswhZry"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}